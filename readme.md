# 一些关于机器学习优化函数的练习
* p1: 最基本的梯度下降法：gradient descent
* p2: 原始 SGD方法：stochastic gradient descent
* p3: minibatch-SGD方法
* p4 momentum SGD: minibatch-SGD with momentum
* p4 momentum: momentum with GD
* p5: Nesterov方法
* p6: adagrad

#### 参考
p1 参考 https://zhuanlan.zhihu.com/p/27297638

p2~pn 参考 http://ruder.io/optimizing-gradient-descent/index.html

###### 单独参考

p5 参考 http://cs231n.github.io/neural-networks-3/

p6 参考 https://zhuanlan.zhihu.com/p/22252270